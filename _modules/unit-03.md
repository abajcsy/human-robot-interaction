---
title: Learning, Alignment, and Safety
---

Oct. 28
: Robot Learning from Human Feedback
  : [Learning from Physical HRI](https://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf), [Correcting Robot Plans with Natural Language Feedback](https://arxiv.org/abs/2204.05186)

Oct. 30
: Robot Learning from Human Feedback
  : **Paper Reading**{: .label .label-red} **Mid-term Report Due**{: .label .label-yellow} 
  [Human-in-the-loop Continual Learning](https://arxiv.org/abs/2211.08416), [Learning Human Objectives by Evaluating Hypothetical Behavior](https://arxiv.org/abs/1912.05652)


Nov. 4
: **No Class**{: .label .label-purple} Democracy Day
  : 


Nov. 6
: Alignment 
  : [Max Alignment, Min Feedback](https://arxiv.org/abs/2412.04835)

Nov. 11
: Active Learning
  : 

Nov. 13
: Active Learning
  : **Paper Reading**{: .label .label-red} [Active Learning from Demonstratons](https://www.ri.cmu.edu/pub_files/2012/5/icra2012.pdf), [Asking Easy Questions](https://arxiv.org/abs/1910.04365)

Nov. 18
: HRI Safety
  : [Safety with Agency](https://arxiv.org/abs/2504.11717), [Robots that Suggest Safe Alternatives](https://arxiv.org/abs/2409.09883v2)

Nov. 20
: HRI Safety
  : **Paper Reading**{: .label .label-red} [Conformalized Teleop](https://arxiv.org/abs/2406.07767), [Robots that Ask for Help](https://arxiv.org/abs/2307.01928)

