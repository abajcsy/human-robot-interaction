---
title: Learning, Alignment, and Safety
---

Oct. 21
: Alignment 
  : 

Oct. 23
: Alignment
  : **Paper Reading**{: .label .label-red} 
  [Deep RL from Preferences](https://arxiv.org/abs/2001.04465), [Learning from Physical HRI](https://arxiv.org/abs/2301.00901)

Oct. 28
: Representation Learning
  : 
  <!-- [Learning from Physical HRI](https://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf), [Correcting Robot Plans with Natural Language Feedback](https://arxiv.org/abs/2204.05186) -->

Oct. 30
: Representation Learning
  : **Paper Reading**{: .label .label-red} **Mid-term Report Due**{: .label .label-yellow} [SIRL](https://arxiv.org/abs/2301.00810), [ALGAE](https://arxiv.org/abs/2409.08212)
  
  <!-- [Human-in-the-loop Continual Learning](https://arxiv.org/abs/2211.08416), [Learning Human Objectives by Evaluating Hypothetical Behavior](https://arxiv.org/abs/1912.05652) -->


Nov. 4
: **No Class**{: .label .label-purple} Democracy Day
  : 


Nov. 6
: HRI in the Era of Foundation Models
  : 
  <!-- **Paper Reading**{: .label .label-red} [Max Alignment, Min Feedback](https://arxiv.org/abs/2412.04835), [FOREWARN](https://arxiv.org/abs/2502.01828) -->

Nov. 11
: **Guest Lecture**{: .label .label-green} Active Learning
  : 

Nov. 13
: Active Learning
  : **Paper Reading**{: .label .label-red} [Active Learning from Demonstratons](https://www.ri.cmu.edu/pub_files/2012/5/icra2012.pdf), [Asking Easy Questions](https://arxiv.org/abs/1910.04365)

Nov. 18
: Safety & Uncertainty in HRI
  : [Safety with Agency](https://arxiv.org/abs/2504.11717)

Nov. 20
: Safety & Uncertainty in HRI
  : **Paper Reading**{: .label .label-red} [Robots that Suggest Safe Alternatives](https://arxiv.org/abs/2409.09883v2), [Robots that Ask for Help](https://arxiv.org/abs/2307.01928)

